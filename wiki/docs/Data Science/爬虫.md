---
title: 爬虫
hidden: true
---


###  1 开发环境配置


##### 请求库的安装

```bash
pip install lxml
pip install beautifulsoup4
pip install requests
pip install selenium
```
###  2 爬虫基础


#### HTTP基本原理

##### URI和URL

URI的全称为Uniform Resource Identifier，即统一资源标识符。URL的全称为Universal Resource Locator，即统一资源定位符。

URL是URI的子集，也就是说每个URL都是URI，但不是每个URI都是URL。

...to be continued
###  3 基本库的使用



####  使用Urllib

Urllib库是Python内置的HTTP请求库([官方文档链接](https://docs.python.org/3/library/urllib.html))， 它包含四个模块：

* <C>request</C>模块，最基本的HTTP请求模块，可以用来模拟发送请求，就像在浏览器里输入网址然后敲击回车一样，只需要给库方法传入URL还有额外的参数，就可以模拟实现这个过程了。
* <C>error</C>模块即异常处理模块，如果出现请求错误，我们可以捕获这些异常，然后进行重试或其他操作保证程序不会意外终止。
* <C>parse</C>模块，用来解析URL。
* <C>robotparser</C>模块，用来解析robots.txt文件。

#####  发送请求

使用Urllib的[request模块](https://docs.python.org/3/library/urllib.request.html)可以方便地实现Request的发送并得到Response。

<hh>urlopen()</hh>

<C>urllib.request</C>模块提供了最基本的构造HTTP请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有处理authenticaton(授权验证)，redirections(重定向)，cookies(浏览器Cookies)以及其它内容。

我们来感受一下它的强大之处，以Python官网为例，我们来把这个网页抓下来:

```Python
import urllib.request
# 打开网页
response = urllib.request.urlopen('https://www.python.org')
# 得到返回的网页内容
print(response.read().decode('utf-8'))
# 查看返回类型： <class 'http.client.HTTPResponse'>
print(type(response))
```

<C>urlopen</C>返回一个<C>HTTPResponse</C>对象，调用对象的<C>read()</C>方法可以得到返回的网页内容，调用<C>status</C>属性就可以得到返回结果的状态码，如200代表请求成功，404代表网页未找到等，调用<C>getheaders()</C>返回响应的头信息。

```Python
>>> response.status
200
>>> response.getheaders()
[('Server', 'nginx'), ('Content-Type', 'text/html; charset=utf-8'), ('X-Frame-Options', 'SAMEORIGIN'), ('x-xss-protection', '1; mode=block'), ('X-Clacks-Overhead', 'GNU Terry Pratchett'), ('Via', '1.1 varnish'), ('Content-Length', '49419'), ('Accept-Ranges', 'bytes'), ('Date', 'Fri, 19 Oct 2018 01:31:34 GMT'), ('Via', '1.1 varnish'), ('Age', '1228'), ('Connection', 'close'), ('X-Served-By', 'cache-iad2144-IAD, cache-bur17551-BUR'), ('X-Cache', 'HIT, HIT'), ('X-Cache-Hits', '1, 1'), ('X-Timer', 'S1539912694.426186,VS0,VE1'), ('Vary', 'Cookie'), ('Strict-Transport-Security', 'max-age=63072000; includeSubDomains')]
```

如果我们想给链接传递一些参数该怎么实现呢？我们首先看一下<C>urlopen()</C>函数的API：

```python
urllib.request.urlopen(url, data=None, [timeout, ]*, 
    cafile=None, capath=None, cadefault=False, context=None)
```

下面我们详细说明下这几个参数的用法。

**data参数**

<C>data</C>参数是可选的，如果要添加<C>data</C>，它要是字节流编码格式的内容，即<C>bytes</C>类型，可以通过<C>bytes()</C>方法进行转化，另外如果传递了这个 <C>data</C>参数，它的请求方式就不再是GET方式请求，而是POST。

```python
import urllib.parse
import urllib.request
# 转码成bytes(字节流)类型
data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
# 请求站点
response = urllib.request.urlopen('http://httpbin.org/post', data=data)
# 返回网页内容
print(response.read())
```

**timeout参数**

<C>timeout</C>参数可以设置超时时间，单位为秒。如果请求超出了设置的这个时间还没有得到响应，就会抛出异常<C>urllib.error.URLError</C>。如果不指定，就会使用全局默认时间。可以通过设置超时时间C>timeout</C>来控制一个网页如果长时间未响应就跳过它的抓取，利用try-except语句就可以实现这样的操作:

```python
try:
    response = urllib.request.urlopen('http://httpbin.org/get', 
        timeout=0.1)
except urllib.error.URLError as e:
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```

<hh>Request</hh>

<C>urlopen()</C>方法也可以接收<C>Request</C>对象。<C>urlopen(url,...)</C>可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入Headers等信息，我们就可以利用更强大的<C>Request</C>类来构建一个请求。

<C>Request</C>类的构造方法：

```python
class urllib.request.Request(url, data=None, headers={}, 
    origin_req_host=None, unverifiable=False, method=None)
```

* <C>url</C>参数是URL。
* <C>data</C>参数如果要传必须传bytes(字节流)类型的，如果是一个字典，可以先用 <C>urllib.parse</C>模块里的<C>urlencode()</C>编码。
* <C>headers</C>参数是Request Headers，字典格式，你可以在构造Request时通过<C>headers</C>参数直接构造，也可以通过调用Request实例的<C>add_header()</C>方法来添加。
* <C>origin_req_host</C>参数指的是请求方的 host 名称或者 IP 地址.
* <C>unverifiable</C>参数指的是这个请求是否是无法验证的，默认是False
* <C>method</C>参数它用来指示请求使用的方法，比如GET，POST，PUT等等

下面看一个具体的例子

```python
from urllib import request, parse

url = 'http://httpbin.org/post'
headers = {
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
    'Host': 'httpbin.org'
}
dict = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict), encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```


<hh>高级用法</hh>

有没有发现，在上面的过程中，我们虽然可以构造 Request，但是一些更高级的操作，比如 Cookies处理，代理设置等操作我们该怎么办？这时我们可以使用Handler。

<C>urllib.request</C>模块里的<C>BaseHandler</C>类，它是所有其他<C>Handler</C>的父类，它提供了最基本的<C>Handler</C>的方法，例如 <C>default_open()</C>、<C>protocol_request()</C>方法等。接下来就有各种 <C>Handler</C>子类继承这个<C>BaseHandler</C>类，举例几个如下：

* <C>HTTPDefaultErrorHandler</C>用于处理 HTTP 响应错误，错误都会抛出 HTTPError 类型的异常。
* <C>HTTPRedirectHandler</C>用于处理重定向。
* <C>HTTPCookieProcessor</C>用于处理 Cookies。
* <C>ProxyHandler</C>用于设置代理，默认代理为空。
* <C>HTTPPasswordMgr</C>用于管理密码，它维护了用户名密码的表。
* <C>HTTPBasicAuthHandler</C>用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。


#####  处理异常

Urllib的error模块定义了由request模块产生的异常。如果出现了问题，request模块便会抛出error模块中定义的异常。

<hh>URLError</hh>
URLError类来自Urllib库的error模块，它继承自OSError类，是error异常模块的基类，由request模块生的异常都可以通过捕获这个类来处理。它具有一个属性reason，即返回错误的原因。

```python
from urllib import request, error
try:
    # 打开不存在的网站
    response = request.urlopen('http://www.pythonfs.org')
except error.URLError as e:
    print(e.reason)
```

<hh>HTTPError</hh>

HTTPError是URLError的子类，专门用来处理HTTP请求错误，比如认证请求失败等等。

它有三个属性:

* code，返回 HTTP Status Code，即状态码，比如404网页不存在，500服务器内部错误等等。
* reason，同父类一样，返回错误的原因。
* headers，返回 Request Headers。

```python
from urllib import request, error
try:
    # 打开不存在的网站
    response = request.urlopen('http://www.pythonfs.org')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers)
```

#####  解析链接
####  分析Robots协议
<hh>Robots协议</hh>

Robots协议也被称作爬虫协议、机器人协议，它的全名叫做网络爬虫排除标准(Robots Exclusion Protocol)，用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫做 robots.txt 的文本文件，放在网站的根目录下，例如[https://www.jd.com/robots.txt](https://www.jd.com/robots.txt) 。

当爬虫访问一个站点时，它首先会检查下这个站点根目录下是否存在robots.txt文件，如果存在，搜索爬虫会根据其中定义的爬取范围来爬取。如果没有找到这个文件，那么搜索爬虫便会访问所有可直接访问的页面。

下面我们看一下京东的robots.txt的样例

```
User-agent: * 
Disallow: /?* 
Disallow: /pop/*.html 
Disallow: /pinpai/*.html?* 
User-agent: EtaoSpider 
Disallow: / 
User-agent: HuihuiSpider 
Disallow: / 
User-agent: GwdangSpider 
Disallow: / 
User-agent: WochachaSpider 
Disallow: /
```

最简单的robots.txt只有两条规则：

* User-agent：指定对哪些爬虫生效
* Disallow：指定要屏蔽的网址。以正斜线(/)开头，可以列出特定的网址或模式。要屏蔽整个网站，使用正斜线即可;要屏蔽某一目录以及其中的所有内容，在目录名后添加正斜线;要屏蔽某个具体的网页，就指出这个网页。

<hh>robotparser</hh>


robotparser模块提供了一个类，叫做RobotFileParser。它可以根据某网站的 robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。

使用非常简单，首先看一下它的声明

```
urllib.robotparser.RobotFileParser(url='')
```

使用这个类的时候非常简单，只需要在构造方法里传入robots.txt的链接即可。当然也可以声明时不传入，默认为空，再使用<C>set_url()</C>方法设置一下也可以。

有常用的几个方法分别介绍一下：

* <C>set_url()</C>，用来设置 robots.txt 文件的链接。如果已经在创建 RobotFileParser 对象时传入了链接，那就不需要再使用这个方法设置了。
* <C>read()</C>，读取 robots.txt 文件并进行分析，注意这个函数是执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为 False，所以一定记得调用这个方法，这个方法不会返回任何内容，但是执行了读取操作。
* <C>parse()</C>，用来解析 robots.txt 文件，传入的参数是 robots.txt 某些行的内容，它会按照 robots.txt 的语法规则来分析这些内容。
* <C>can_fetch()</C>，方法传入两个参数，第一个是 User-agent，第二个是要抓取的 URL，返回的内容是该搜索引擎是否可以抓取这个 URL，返回结果是 True 或 False。
* <C>mtime()</C>，返回的是上次抓取和分析 robots.txt 的时间，这个对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的 robots.txt。
* <C>modified()</C>，同样的对于长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析 robots.txt 的时间。

以上是这个类提供的所有方法，下面我们用实例来感受一下：


```
from urllib.robotparser import RobotFileParser
rp = RobotFileParser()
rp.set_url('https://www.jd.com/robots.txt')
rp.read()
print(rp.can_fetch('*', 'https://miaosha.jd.com')) // True
print(rp.can_fetch('GwdangSpider', "https://miaosha.jd.com")) // False
```


#### 使用Requests

在前面一节我们了解了<C>Urllib</C>的基本用法，但是其中确实有不方便的地方。比如处理网页验证、处理Cookies等等，需要写Opener、Handler来进行处理。为了更加方便地实现这些操作，在这里就有了更为强大的库<C>Requests</C>，有了它，Cookies、登录验证、代理设置等操作都不是事儿。

[Requests官方文档](http://docs.python-requests.org/)

##### GET请求

首先让我们来构建一个最简单的GET请求，请求的链接为：https://www.zhihu.com.

```Python
import requests
import re
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) 
    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116
     Safari/537.36'
}
r = requests.get("https://www.zhihu.com/explore", headers=headers)
pattern = re.compile('explore-feed.*?question_link.*?>(.*?)</a>', re.S)
titles = re.findall(pattern, r.text)
print(titles)
```

##### POST请求

```Python
import requests

data = {'name': 'germey', 'age': '22'}
r = requests.post("http://httpbin.org/post", data=data)
print(r.text)
```

##### 文件上传

我们知道 Reqeuests 可以模拟提交一些数据，假如有的网站需要我们上传文件，我们同样可以利用它来上传，实现非常简单，实例如下：

```python
import requests

files = {'file': open('favicon.ico', 'rb')}
r = requests.post('http://httpbin.org/post', files=files)
print(r.text)
```

##### Cookie

可以直接用Cookies来维持登录状态。登录知乎，将Headers中的Cookies复制下来，将其设置到Headers里面，发送Request。

```Python
headers = {
    'Cookie': '知乎的Cookie',   # 加入Cookie
    'Host': 'www.zhihu.com',
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) 
    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.116
     Safari/537.36',
}
r = requests.get('https://www.zhihu.com', headers=headers)
print(r.text)
```

##### 会话维持

在Requests中，如果直接利用<C>get()</C>或<C>post()</C>等方法的确可以做到模拟网页的请求。但是这实际上是相当于不同的会话，即不同的<C>Session</C>，也就是说相当于你用了两个浏览器打开了不同的页面。

设想这样一个场景，我们第一个请求利用了<C>post()</C>方法登录了某个网站，第二次想获取成功登录后的自己的个人信息，你又用了一次<C>get()</C> 方法去请求个人信息页面。实际上，这相当于打开了两个浏览器，是两个完全不相关的会话，能成功获取个人信息吗？那当然不能。

其实解决这个问题的主要方法就是维持同一个会话，也就是相当于打开一个新的浏览器选项卡而不是新开一个浏览器。但是我又不想每次设置Cookies，那该怎么办？这时候就有了新的利器<C>Session</C>对象。


```Python
s = requests.Session()
s.get('http://httpbin.org/cookies/set/number/123456789')
r = s.get('http://httpbin.org/cookies')
print(r.text)
```

##### 认证

HTTP基本认证： 可以传递一个HTTPBasicAuth类，也可以传递一个元祖，它会默认使用HTTPBasicAuth这个类来认证。


```Python
from requests.auth import HTTPBasicAuth
r = requests.get(url, auth=HTTPBasicAuth('username', 'password'))
r = requests.get(url,  auth=('username', 'password'))
```

 OAuth认证，需要安装[Requests-oAuthlib](https://requests-oauthlib.readthedocs.io/en/latest/)包：

```bash
pip install requests_oauthlib
```

#####  代理设置

对于某些网站，在测试的时候请求几次，能正常获取内容。但是一旦开始大规模爬取，对于大规模且频繁的请求，网站可能会直接登录验证，验证码，甚至直接把IP给封禁掉。那么为了防止这种情况的发生，我们就需要设置代理来解决这个问题，在Requests中需要用到proxies这个参数。

```Python
proxies = {
  'http': 'http://10.10.1.10:3128',
  'https': 'http://10.10.1.10:1080',
}
requests.get('https://www.taobao.com', proxies=proxies)
```


#### 正则表达式

#### Example: 抓取猫眼电影排行




###  4 解析库的使用



使用正则表达式提取页面信息是比较繁琐的，而且容易出现错误。对于网页节点来说，它可以定义id、class或其他的属性，而且节点之间具有层次关系。利用XPath或CSS选择器可以来定位节点，如果我们利用选择器提取节点，然后再获取它的内容或者属性不就可以非常方便的提取信息了吗？

在Python中，我们怎样来实现这个操作呢？Python有强大的解析库：LXML、BeautifulSoup、PyQuery等等。

####  XPath的使用
####  BeautifulSoup的使用

[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)是一个强大的解析工具，它借助网页的结构和属性等特性来解析网页。

> Beautiful Soup提供一些简单的、Python式的函数来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。
> Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为UTF-8编码。你不需要考虑编码方式，除非文档没有制定一个编码方式，这时你仅仅需要说明一下原始编码方式就可以了。

##### 基本用法

```Python
from bs4 import BeautifulSoup
import requests
r = requests.get('http://www.datasciencecourse.org/lectures/')
soup = BeautifulSoup(r.text, 'html.parser')
soup.title
# <title>Lectures</title>
soup.title.name
# u'title'
soup.title.string
# u'Lectures'
soup.title.parent.name
# u'head'
soup.p
# <p><i class="fa fa-calendar" style="font-size: 128px"></i>
# <h1>Lectures</h1></p>
# 获取a标签
soup.a
# <a class="navbar-brand" href="http://practicaldatascience.github.io">
# Practical Data Science</a>
# 提取所有的a标签
soup.find_all('a')
# 从a标签提取url
for link in soup.find_all('a'):
    print(link.get('href'))
# 获取页面的所有文字
print(soup.get_text())
```


| 操作 |  |
| --- | --- |
| 选择元素 | soup.title, soup.head, soup.p, soup.a |
| 获取元素名称 | soup.title.name, soup.head.name, soup.a.name  |
| 获取属性 | soup.p['class'], soup.p['name']  |
| 获取内容 | soup.title.string, soup.p.string|
| 获取属性 | soup.p['class'], soup.p['name']  |
| 获取子孙 | soup.p.children, soup.p.descendants  |
| 获取父节点 | soup.a.parent, soup.a.parents  |
| 获取兄弟节点 | soup.a.next_sibling, soup.a.previous_siblings  |
| 查询所有符合条件的元素 | soup.find_all()  |
| 查询符合条件的第一个元素 | soup.find()  |


##### CSS选择器

Beautiful Soup还提供了[CSS选择器](CSS.md)。使用CSS选择器时，只需要调用<C>select()</C>方法，传入相应的CSS选择器即可。

```Python
soup.select('title')
# [<title>Lectures</title>]
soup.select('table')
```
用<C>select()</C>方法返回的是一个列表，如果只需要返回一个结果，可以用<C>select_one()</C>方法


在chrome浏览器中，点击右键，选择`copy-copy selector`即可。






####  PyQuery的使用
###  5 数据存储


...to be continued
###  6 Ajax数据爬取


...to be continued

###  7 动态渲染页面爬取


我们可以直接使用模拟浏览器运行的方式来实现爬取，这样就可以做到在浏览器中看到是什么样，抓取的源码就是什么样，也就是可见即可爬。

Selenium是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作，如点击、下拉等操作，同时还可以获取浏览器当前呈现的页面的源代码 ，做到可见即可爬。 对于一些JavaScript动态渲染的页面来说，此种抓取方式非常有效。

#### Selenium的使用

Selenium在使用前需要安装Selenium库和Chrome Drive.

##### 声明浏览器对象

Selenium支持非常多的浏览器，如Chrome、Firefox等. 


```Python
from selenium import webdriver
browser = webdriver.Chrome()
browser = webdriver.Safari()
```

这样我们就完成了浏览器对象的初始化并赋值为browser对象，接下来我们要做的就是调用browser对象，让其执行各个动作，就可以模拟浏览器操作了。

##### 访问页面

我们可以用`get()`方法来请求一个网页，参数传入链接URL即可，比如在这里我们用`get()`方法访问淘宝，然后打印出源代码，代码如下：

```python
browser.get('https://www.taobao.com')
print(browser.page_source)
browser.close()
```

运行之后我们便发现弹出了Chrome浏览器，自动访问了淘宝，然后控制台输出了淘宝页面的源代码，随后浏览器关闭。


##### 查找节点

Selenium 提供了一系列查找节点的方法。 比如，

* `find_element_by_name()`是根据name值获取.
* `find_element_by_id()`是根据id获取。
* `find_elements_by_xpath`是根据Xpath获取。
* `find_elements_by_class_name`是根据class name获取。
* `find_elements_by_css_selector`是根据css selector获取。

例如，想要从淘宝页面中提取搜索框这个节点，

```python
input_first = browser.find_element_by_id('q')
input_second = browser.find_element_by_css_selector('#q')
input_third = browser.find_element_by_xpath('//*[@id="q"]')
```

另外Selenium还提供了通用的`find_element()`方法，它需要传入两个参数，一个是查找的方式 `By`，另一个就是值，实际上它就是`find_element_by_id/name/xpath()`这种方法的通用函数版本，比如`find_element_by_id(id)`就等价于`find_element(By.ID, id)`，二者得到的结果完全一致。

如果要查找所有满足条件的节点，而不是一个节点，那就需要用`find_elements()`这样的方法，方法名称中element多了一个s，注意区分。

例如查找淘宝左侧导航条的所有条目

```python
lis = browser.find_elements_by_css_selector('.service-bd li')
```

和刚才一样，也可可以直接`find_elements()`方法来选择，所以也可以这样来写：

```python
lis = browser.find_elements(By.CSS_SELECTOR, '.service-bd li')
```


##### 获取节点信息

我们可以使用`get_attribute()`方法来获取节点的属性，那么这个的前提就是先选中这个节点。

每个WebEelement节点都有text属性，我们可以通过直接调用这个属性就可以得到节点内部的文本信息了，就相当于BeautifulSoup的`get_text()`方法、PyQuery的`text()`方法。

另外 WebElement 节点还有一些其他的属性，比如id属性可以获取节点id，location 可以获取该节点在页面中的相对位置，tag_name 可以获取标签名称，size 可以获取节点的大小，也就是宽高，这些属性有时候还是很有用的。


##### 节点交互

Selenium可以驱动浏览器来执行一些操作，
     
输入文字用`send_keys()`方法，清空文字用`clear()`方法，另外还有按钮点击，用`click()`方法。


##### 延时等待

在Selenium中, `get()`方法会在网页框架在结束后结束执行，此时如果获取page_source，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到。

所以需要延时等待一定时间，确保节点已经加载出来。有两种等待方式：隐式等待，显式等待。

###### 隐式等待

当使用隐式等待执行测试的时候，如果Selenium没有在DOM中找到节点，将继续等待，超出设定时间后，则抛出找不到节点的异常。

```Python
from selenium import webdriver
browser = webdriver.Chrome()
browser.implicitly_wait(10) 
browser.get(url)
browser.find_element_by_css_selector('dsa')
```

###### 显式等待

更好的方法是指定一个节点和最长等待时间。如果在规定时间内加载出了这个节点，就返回查找的节点；如果到了规定时间依然没有加载出该节点，则抛出超时异常。

```Python
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

browser = webdriver.Chrome()
browser.get(url)
wait = WebDriverWait(browser, 10)
wait.until(EC.element_to_be_clickable(By.CLASS_NAME, 'reactable-next-page')).
```

* frame可见并切换到该frame上
`EC.frame_to_be_available_and_switch_to_it`
* 元素可以点击，常用于按键
`EC.element_to_be_clickable`
* 元素出现，只要一个符合条件的元素加载出来就通过
`EC.presence_of_element_located`
* 元素出现，须所有符合条件的元素都加载出来，这个基本上就是你爬取的最主要内容了
`EC.presence_of_all_elements_located`
* 判断某段文本是否出现在某元素中，常用于判断输入页数与实际高亮页数是否一致
`EC.text_to_be_present_in_element`


##### 前进和后退

Selenium使用`back()`方法后退，使用`forward()`方法前进。

##### 选项卡管理

连续调用`browser.get()方法，在访问页面的时候，会默认启动同一个选项卡。在Selenium中，可以对选项卡进行操作。

```Python
browser.get(url)
browser.execute_script('window.open()')  #开启新选项卡
browser.switch_to_window(browser.window_handles[1]) # 切换到新选项卡
```


##### Application: LeetCode

```Python
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
import json
import codecs
import time

def parse(browser, file):
	for element in browser.find_elements_by_xpath('//*[@id="question-app"]/div/div[2]/div[2]/div[2]/table/tbody[1]/tr'):
		id, title, difficulty = element.text.split("\n")
		url = element.find_element_by_xpath('td[3]/div/a').get_attribute("href")
		data = {'id': id.strip(), 'title': title.strip(), 'difficulty': difficulty.strip(), 'url': url}
		print(data)
		file.write(json.dumps(data) + ",\n")
```


###  8 验证码的识别


...to be continued
###  9 代理的使用


...to be continued

###  10 模拟登录


...to be continued
###  11 App的爬取


...to be continued
###  12 pyspider框架的使用


...to be continued
###  13 Scrapy框架的使用



#### Scrapy框架介绍
[Scrapy](https://scrapy.org/)是一个基于[Twisted](https://twistedmatrix.com/trac/)的异步处理框架，是纯Python实现的爬虫框架，其架构清晰，模块之间的耦合程度低，可扩展性极强，可以灵活完成各种需求。

![](figures/ScrapyStructure.jpg)

* Engine。 引擎，处理整个系统的数据流处理、触发事务，是整个框架的核心。
* Item。 项目，它定义了爬取结果的数据结构，爬取的数据会被赋值成该Item对象。 
* Scheduler。调度器，接受引擎发过来的请求并将其加入队列中，在引擎再次请求的时候将请求提供给引擎。
* Downloader。下载器，下载网页内容，并将网页内容返回给蜘蛛。
* Spiders。 蜘蛛，其内定义了爬取的逻辑和网页的解析规则，结果和新的请求。 
* Item Pipeline。 项目管道, 负责处理由蜘蛛从网页中抽取的项目，它主要负责解析响应并生成提取. 它的主要任务是清洗 、验证和存储数据。 
* Downloader Middlewares。 下载器中间件, 位于引擎和下载器之间的组件，主要处理引擎与下载器之间的请求及响应。 
* Spider Middlewares。 蜘蛛中间件，位于引擎和蜘蛛之间的组件，主要处理蜘蛛输入的响应和输出的结果及新的请求。

Scrapy中的数据流由引擎控制，数据流的过程如下。

1. Engine 首先打开一个网站， 找到处理该网站的 Spider ，并向该 Spider 请求第一个要爬取的 URL。
2. Engine从Spider中获取到第一个要爬取的URL，并通过Scheduler以<C>Request</C>的形式调度 。
3. Engine向Scheduler请求下一个要爬取的URL。
4. Scheduler返回下一个要爬取的URL给Engine, Engine将URL通过Downloader Middlewares转发给 Downloader下载。 
5. 一旦页面下载完毕， Downloader生成该页面的<C>Response</C>， 并将其通过Downloader Middlewares发送给Engine。
6. Engine从下载器中接收到<C>Response</C> ， 并将其通过Spider Middlewares发送给Spider处理。
7. Spider处理 Response，并返回爬取到的Item及新的<C>Request</C>给Engine。
8. Engine将Spider返回的Item给ItemPipeline，将新的Request给Scheduler。
9. 重复第（2）步到第（8 ）步， 直到Scheduler中没有更多的<C>Request</C> , Engine关闭该网站， 爬取结束。

#### Scrapy入门

创建一个Scrapy项目，项目文件可以直接用scrapy命令生成

```bash
scrapy startproject projectname
```

创建之后，项目文件结构如下所示 ：

```
scrapy.cfg 
project/
    __init__. py
    items.py 
    pipelines.py
    settings.py 
    middlewares.py 
    spiders/
        init.py 
```

Spider是自己定义的类， scrapy用它来从网页里抓取内容， 并解析抓取的结果。 不过这个类必须继承scrapy提供的Spider类<C>scrapy.Spider</C>， 还要定义Spider的名称和起始请求， 以及怎样处理爬取后的结果的方法。

也可以使用命令行创建一个Spider。 比如要生成Quotes这个Spider，可以执行如下命令： 

```
cd projectname 
scrapy genspider quotes quotes.toscrape.com
```
genspider命令的第一个参数是Spider的名称， 第二个参数是网站域名。执行完毕之后，spiders文件夹中多了一个quotes.py，它就是刚刚创建的Spider, 内容如下所示： 

```
class QuotesdemoSpider(scrapy.Spider):
    name = 'quotesdemo'
    allowed_domains = ['quotes.toscrape.com']
    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        pass
```
这里有三个属性 -- <C>name</C>, <C>allowed_domains</C>, <C>start_urls</C>, 还有一个方法 <C>parse</C>。

* <C>name</C>, 它是每个项目唯一的名字， 用来区分不同的Spider。
* <C>allowed_domains</C>, 它是允许爬取的域名，如果初始或后续的请求链接不是这个域名下的，则请求链接会被过滤掉。 
* <C>start_urls</C>，它包含了Spider在启动时爬取的url列表，初始请求是由它来定义的 。 
* <C>parse</C>，它是<C>Spider</C>的一个方法。 默认情况下，被调用时<C>start_urls</C>里面的链接构成的请求完成下载执行后， 返回的响应就会作为唯一的参数传递给这个函数。 该方法负责解析返回的响应、提取数据或者进一步生成要处理的请求。

#### Scrapy对接Selenium

Scrapy抓取页面的方式和requests库类似，都是直接模拟HTTP请求，所以Scrapy也不能抓取JavaScript动态渲染的页面。直接用Selenium模拟浏览器进行抓取，可见即可爬。

本节以抓取淘宝商品信息为例子，演示Scrapy框架如何对接Selenium。首先新建项目和Spider，修改`settings.py`中的`ROBOTSTEXT_OBEY=False`。

```bash
scrapy startproject scrapyseleniumtest
scrapy genspider taobao www.taobao.com
```

然后定义Item对象，名为<C>ProductItem</C>

```python
class ProductItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    collection = 'products'
    image = scrapy.Field()
    price = scrapy.Field()
    deal = scrapy.Field()
    titlte = scrapy.Field()
    shop = scrapy.Field()
    location = scrapy.Field()
```

这里我们定义了6个<C>Field</C>，然后定义了一个<C>collection</C>属性，即此<C>Item</C>保存的MongoDB的<C>Collection</C>名称。

初步实现Spider的<C>start_requests()</C>方法:

```python
class TaobaoSpider(scrapy.Spider):
	name = 'taobao'
	allowed_domains = ['taobao.com']
	base_url = ['https://s.taobao.com/search?q=']

	def start_requests(self):
		for keyword in self.settings.get("KEYWORDS"):
			for page in range(1, self.settings.get("MAX_PAGE") + 1):
				url = self.base_url + quote(keyword)
				yield scrapy.Request(url=url, callback=self.parse, 
				    meta={'page': page}, dont_filter=True)
```

首先定义了商品列表的URL-<C>base_url</C>, 其后拼接一个搜索关键字就是该关键字在淘宝搜索结果商品列表页面。关键字用KEYWORDS标识，定义为一个列表，最大翻页页码用MAX_PAGE表示。他们统一定义在`settings.py`里面，

```Python
KEYWORDS = ['iPad']
MAX_PAGE = 100
```

接下来我们需要处理这些请求的抓取：对接Selenium进行抓取，采用Downloader Middleware来实现。在Middleware里面的<C>process_request()</C>方法里对每个抓取请求进行处理，启动浏览器并进行页面渲染，再将渲染后的结果构造一个<C>HtmlResponse</C>对象返回。

 ```Python

 ```







###  14 分布式爬虫


...to be continued
###  15 分布式爬虫的部署


...to be continued